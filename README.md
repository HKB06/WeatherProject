# ‚òÅÔ∏è WeatherProject - Big Data Pipeline

Pipeline d'analyse m√©t√©orologique Big Data avec Apache Spark, PostgreSQL et Flask.

---

## üìã Contexte & Choix du Dataset

Ce projet s'appuie sur des **donn√©es m√©t√©orologiques r√©elles** de la C√¥te d'Azur, couvrant la p√©riode **2023-2025** (plus de 1000 jours).

Chaque enregistrement contient :
- **Date** de l'observation
- **Temp√©ratures** (minimale, maximale, moyenne)
- **Humidit√© relative** moyenne
- **Pr√©cipitations** cumul√©es
- **Vitesse du vent** maximale
- **Localisation** (5 villes : Nice, Cannes, Monaco, Antibes, Menton)

### Pourquoi ce dataset ?

- **Dimension temporelle** : id√©al pour analyser les tendances saisonni√®res et l'√©volution du climat
- **Dimension g√©ographique** : comparaison entre 5 villes de la C√¥te d'Azur
- **Donn√©es publiques** : provenant d'Open-Meteo Archive API (source officielle)
- **Pertinence m√©tier** : applications en tourisme, agriculture, pr√©vention des risques

---

## üéØ Questions Analytiques

Le projet r√©pond √† plusieurs questions d'analyse m√©t√©orologique :

### 1. Comment √©voluent les temp√©ratures dans le temps ?
- Quelle est la **temp√©rature moyenne** sur 3 ans ?
- Observe-t-on une **tendance** √† la hausse ou √† la baisse ?
- Quelles sont les **variations saisonni√®res** ?

### 2. Quelles sont les p√©riodes les plus extr√™mes ?
- Quel **mois** enregistre les temp√©ratures les plus √©lev√©es/basses ?
- Quelle **saison** est la plus pluvieuse ?
- Quelle **ann√©e** a √©t√© la plus chaude ?

### 3. Existe-t-il des diff√©rences entre les villes de la C√¥te d'Azur ?
- Quelle ville a la **temp√©rature moyenne** la plus √©lev√©e ?
- Quelle ville re√ßoit le **plus de pr√©cipitations** ?
- Les **tendances** sont-elles similaires ou diff√©rentes entre les villes ?

### 4. Comment se r√©partissent les temp√©ratures ?
- Quelle est la **distribution** des temp√©ratures journali√®res ?
- Combien de jours d√©passent **30¬∞C** (canicule) ?
- Combien de jours sont en dessous de **5¬∞C** (froid) ?

### 5. Quels sont les patterns saisonniers ?
- Quelle est la **temp√©rature moyenne par saison** ?
- La **variabilit√©** est-elle plus forte en √©t√© ou en hiver ?
- Observe-t-on des **anomalies** saisonni√®res ?

---

## üèóÔ∏è Architecture du Projet

L'architecture suit une logique de **DataLake en 3 couches** : Ingestion, Persistance, Insight.

### 1. Ingestion

**Source de donn√©es** : Open-Meteo Archive API (gratuite, sans cl√©)

- **Type 1** : API REST (JSON) - donn√©es temps r√©el/historiques
- **Type 2** : Conversion CSV - donn√©es structur√©es pour Spark

**Processus** :
- Appel API pour 5 villes (Nice, Cannes, Monaco, Antibes, Menton)
- P√©riode : 2023-01-01 ‚Üí 2025-11-22 (aujourd'hui)
- Sauvegarde brute : `data/raw/*.json` et `data/raw/*.csv`
- R√©silience : checkpoints, retry, logs

### 2. Persistance (DataLake)

Les donn√©es sont organis√©es en **3 zones** :

#### Zone RAW (donn√©es brutes)
- **Localisation** : `data/raw/`
- **Formats** : JSON (API) + CSV (conversion)
- **Contenu** : Donn√©es brutes telles que re√ßues de l'API
- **Conservation** : Toutes les donn√©es sources sont conserv√©es

#### Zone CURATED (donn√©es nettoy√©es)
- **Traitement Spark** :
  - Nettoyage des valeurs manquantes
  - Validation des donn√©es (temp√©ratures coh√©rentes, dates valides)
  - Conversion des types (dates, nombres)
  - Ajout de colonnes d√©riv√©es (ann√©e, mois, saison)

#### Zone ANALYTICS (donn√©es agr√©g√©es)
- **Localisation** : `data/processed/`
- **Format** : Parquet (optimis√©, compress√©)
- **Datasets g√©n√©r√©s** :
  - `daily/daily.parquet` - Agr√©gations journali√®res (1057 jours)
  - `monthly/monthly.parquet` - Agr√©gations mensuelles (35 mois)
  - `seasonal/seasonal.parquet` - Agr√©gations saisonni√®res (12 saisons)

#### M√©tadonn√©es (PostgreSQL)
- Table `ingestion_metadata` : tra√ßabilit√© des ingestions
- Table `processing_metadata` : statistiques de traitement
- S√©paration stricte donn√©es/m√©tadonn√©es

### 3. Insight (Dashboard & Visualisation)

**Infrastructure Docker** :
- **PostgreSQL** : M√©tadonn√©es et tra√ßabilit√©
- **Spark Master** : Coordination du traitement distribu√©
- **Spark Worker** : Ex√©cution des t√¢ches Spark
- **Dashboard Flask** : Application web de visualisation

**Fonctionnalit√©s du Dashboard** :
- Statistiques globales (temp√©rature, pr√©cipitations, humidit√©)
- Graphique d'√©volution temporelle avec **filtres dynamiques** (30j, 1an, 2ans, 3ans)
- Moyennes mensuelles et saisonni√®res
- Heatmap des temp√©ratures par mois
- Distribution des temp√©ratures
- Analyse des pr√©cipitations
- Export de donn√©es

---

## üîÑ Pipeline End-to-End

Le script `run_pipeline.py` orchestre la pipeline compl√®te :

### √âtape 1 : Ingestion des Donn√©es
```
Appel API Open-Meteo Archive
  ‚Üì
Sauvegarde JSON (data/raw/)
  ‚Üì
Conversion CSV pour Spark
  ‚Üì
Sauvegarde m√©tadonn√©es PostgreSQL
```

### √âtape 2 : Traitement Spark
```
Lecture CSV brut
  ‚Üì
Nettoyage & Validation (5285 ‚Üí 5285 records)
  ‚Üì
Enrichissement (colonnes d√©riv√©es)
  ‚Üì
Agr√©gations :
  - Journali√®res (1057 jours)
  - Mensuelles (35 mois)
  - Saisonni√®res (12 saisons)
  ‚Üì
Calcul des tendances
```

### √âtape 3 : Persistance
```
Conversion Spark ‚Üí Pandas
  ‚Üì
Sauvegarde Parquet (compression Snappy)
  ‚Üì
Sauvegarde m√©tadonn√©es PostgreSQL
```

### √âtape 4 : Visualisation
```
Dashboard Flask lit les Parquet
  ‚Üì
API REST pour les donn√©es
  ‚Üì
Graphiques interactifs (Plotly.js, Chart.js)
```

**Dur√©e totale** : ~35-40 secondes

---

## üöÄ Pr√©requis

- **Git**
- **Docker** et **Docker Compose**
- **Python** 3.11+ (pour ex√©cution locale du pipeline)
- **Connexion Internet** (t√©l√©chargement des donn√©es)

---

## üì• Installation & Lancement

### 1. Cloner le projet

```bash
git clone https://github.com/HKB06/WeatherProject.git
cd WeatherProject
```

### 2. Cr√©er le fichier `.env`

```bash
# Windows
copy .env.example .env

# Linux/Mac
cp .env.example .env
```

Contenu minimal du `.env` :
```env
POSTGRES_HOST=postgres
POSTGRES_PORT=5432
POSTGRES_DB=weather_metadata
POSTGRES_USER=weather_admin
POSTGRES_PASSWORD=weather_pass_2025
```

### 3. Cr√©er l'environnement virtuel Python (optionnel, pour ex√©cution locale)

**Windows** :
```bash
python -m venv env
env\Scripts\activate
pip install -r requirements.txt
```

**Linux/Mac** :
```bash
python -m venv env
source env/bin/activate
pip install -r requirements.txt
```

### 4. D√©marrer l'infrastructure Docker

```bash
docker-compose up -d
```

**Services d√©marr√©s** :
- PostgreSQL (port 5432)
- Spark Master (port 8080)
- Spark Worker
- Dashboard Flask (port 5000)

### 5. Lancer la pipeline

```bash
python run_pipeline.py
```

**Sortie attendue** :
```
PHASE 1: INGESTION DES DONN√âES R√âELLES
  ‚úì 5 villes ing√©r√©es (Nice, Cannes, Monaco, Antibes, Menton)
  ‚úì 5285 enregistrements bruts

PHASE 2: TRAITEMENT SPARK
  ‚úì 1057 jours agr√©g√©s
  ‚úì 35 mois agr√©g√©s
  ‚úì 12 saisons agr√©g√©es

PHASE 3: PERSISTANCE DES DONN√âES
  ‚úì Parquet sauvegard√©s (0.07 MB)
  ‚úì M√©tadonn√©es PostgreSQL OK

PIPELINE TERMIN√â AVEC SUCC√àS (35.8s)
```

### 6. Acc√©der au dashboard

**Dashboard** : http://localhost:5000

**Spark UI** : http://localhost:8080

---

## üìä Utilisation du Dashboard

Le dashboard Flask propose plusieurs visualisations interactives :

### 1. Vue d'Ensemble
- **Temp√©rature moyenne** : 17.0¬∞C
- **Pr√©cipitations totales** : 12 910 mm
- **Humidit√© moyenne** : 69.5%
- **P√©riode d'analyse** : 1057 jours

### 2. √âvolution de la Temp√©rature
- **Graphique interactif** (Plotly.js) avec 3 courbes :
  - Temp√©rature moyenne (ligne bleue)
  - Temp√©rature maximale (ligne rouge pointill√©e)
  - Temp√©rature minimale (ligne cyan pointill√©e)
- **Filtres dynamiques** :
  - 30 derniers jours
  - 90 derniers jours
  - 1 an
  - 2 ans
  - 3 ans (complet)
- **Interaction** : Zoom, pan, hover pour d√©tails

### 3. Analyses Temporelles
- **Moyennes mensuelles** : temp√©rature et pr√©cipitations par mois
- **Comparaison saisonni√®re** : graphique radar par saison
- **Heatmap** : temp√©ratures par mois (2023-2025)

### 4. Analyses Avanc√©es
- **Pr√©cipitations mensuelles** : bar chart interactif
- **Distribution des temp√©ratures** : histogramme (Chart.js)
- Identification des **anomalies climatiques**

### 5. Gestion des Donn√©es
- **Filtres** :
  - Date de d√©but
  - Date de fin
  - Limite de r√©sultats
- **Table interactive** : tri, recherche, pagination
- **Export CSV** : t√©l√©chargement des donn√©es filtr√©es

---

## üõ†Ô∏è Technologies & Stack Technique

### Big Data
- **Apache Spark 3.3.0** - Traitement distribu√©
- **PySpark** - API Python pour Spark
- **Parquet** - Format de stockage optimis√©

### Base de Donn√©es
- **PostgreSQL 16** - M√©tadonn√©es et tra√ßabilit√©
- **psycopg2** - Driver Python pour PostgreSQL

### Web & Visualisation
- **Flask 3.0** - Framework web Python
- **Plotly.js** - Graphiques interactifs
- **Chart.js** - Visualisations compl√©mentaires

### Orchestration
- **Docker** - Containerisation
- **Docker Compose** - Orchestration multi-conteneurs

### Ingestion
- **Open-Meteo Archive API** - Donn√©es m√©t√©orologiques officielles
- **Requests** - Client HTTP Python

---

## ‚öôÔ∏è Configuration Avanc√©e

### Modifier les villes analys√©es

√âditer `config/config.yaml` :

```yaml
data_sources:
  api:
    cities:
      - name: "Nice"
        latitude: 43.7102
        longitude: 7.2620
      # Ajouter d'autres villes ici
```

### Changer la p√©riode d'analyse

Dans `src/ingestion/api_ingestion.py` :

```python
start_date = datetime(2023, 1, 1).date()  # Modifier l'ann√©e
end_date = datetime.now().date()
```

### Ajuster les ressources Spark

Dans `docker-compose.yml` :

```yaml
spark-worker:
  environment:
    - SPARK_WORKER_CORES=4      # Nombre de cores
    - SPARK_WORKER_MEMORY=4G    # M√©moire allou√©e
```

---

## üîç Conformit√© avec le Cahier des Charges

### Exigences TP ‚Üí Impl√©mentation

| Exigence | Impl√©mentation |
|----------|----------------|
| **2 sources de donn√©es diff√©rentes** | ‚úÖ API REST (JSON) + CSV historiques |
| **Ingestion r√©siliente** | ‚úÖ Checkpoints, retry, logs d√©taill√©s |
| **Donn√©es brutes conserv√©es** | ‚úÖ `data/raw/` (JSON + CSV) |
| **M√©tadonn√©es s√©par√©es** | ‚úÖ PostgreSQL (tables d√©di√©es) |
| **ETL complet** | ‚úÖ Spark (Extract, Transform, Load) |
| **Indexation** | ‚úÖ Parquet avec compression Snappy |
| **Dashboard interactif** | ‚úÖ Flask + Plotly.js (filtres dynamiques) |
| **Framework Big Data** | ‚úÖ Apache Spark distribu√© |
| **Insights significatifs** | ‚úÖ Tendances, saisonnalit√©, anomalies |
| **Architecture DataLake** | ‚úÖ 3 couches (RAW, CURATED, ANALYTICS) |

---

## üìâ Limites & Pistes d'Am√©lioration

### Limites actuelles

1. **G√©ographie limit√©e** : Seulement la C√¥te d'Azur (pas de comparaison nationale/europ√©enne)
2. **Traitement batch** : Pas de streaming temps r√©el
3. **Mod√®le statistique simple** : Pas de pr√©diction m√©t√©o (Machine Learning)
4. **API gratuite** : Limites de fr√©quence et de granularit√©
5. **Pas de d√©tection d'anomalies avanc√©e** : Approche purement descriptive

### Am√©liorations possibles

1. **√âtendre g√©ographiquement** :
   - Ajouter d'autres r√©gions fran√ßaises
   - Comparaison inter-r√©gions

2. **Streaming temps r√©el** :
   - Int√©gration Kafka + Spark Streaming
   - Mise √† jour automatique du dashboard

3. **Machine Learning** :
   - Pr√©diction des temp√©ratures (LSTM, Prophet)
   - D√©tection d'anomalies climatiques (Isolation Forest)
   - Clustering de patterns m√©t√©o

4. **Enrichissement des donn√©es** :
   - Ajout de la qualit√© de l'air
   - Donn√©es satellite
   - √âv√©nements m√©t√©o extr√™mes

5. **Optimisations techniques** :
   - Partitionnement Parquet par ann√©e/mois
   - Cache Redis pour le dashboard
   - API REST pour int√©gration externe

6. **Analyses avanc√©es** :
   - Corr√©lation avec donn√©es touristiques
   - Impact sur l'agriculture locale
   - Analyse pr√©dictive des canicules

---

## üõë Arr√™t du Projet

```bash
# Arr√™ter tous les conteneurs
docker-compose down

# Supprimer les volumes (attention : perte des m√©tadonn√©es)
docker-compose down -v
```

---

## üë• Auteurs

**Hugo K.** 

---

## üîó Liens Utiles

- [Open-Meteo Archive API](https://open-meteo.com/en/docs/historical-weather-api)
- [Apache Spark Documentation](https://spark.apache.org/docs/3.3.0/)
- [Flask Documentation](https://flask.palletsprojects.com/)
- [Plotly.js Documentation](https://plotly.com/javascript/)
