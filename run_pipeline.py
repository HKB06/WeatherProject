"""Pipeline complet WeatherProject: Ingestion -> Traitement -> Persistance"""

import sys
import time
from pathlib import Path
from datetime import datetime
from dotenv import load_dotenv
import os

load_dotenv()
sys.path.append(str(Path(__file__).parent))

from src.utils.logger import get_logger
from src.ingestion.csv_ingestion import NOAACSVIngestion
from src.ingestion.api_ingestion import WeatherAPIIngestion
from src.processing.spark_processing import WeatherSparkProcessor
from src.persistence.data_persistence import DataPersistence

logger = get_logger(__name__)


def print_banner():
    banner = """
    ================================================================
              WEATHERPROJECT - BIG DATA PIPELINE             
               Architecture DataLake avec Apache Spark          
    ================================================================
    """
    try:
        print(banner)
    except UnicodeEncodeError:
        print("WEATHERPROJECT - BIG DATA PIPELINE")
        print("Architecture DataLake avec Apache Spark")


def run_ingestion_phase():
    logger.info("=" * 80)
    logger.info("PHASE 1: INGESTION DES DONN√âES")
    logger.info("=" * 80)
    
    start_time = time.time()
    
    logger.info("\nüì• Ingestion CSV - Donn√©es historiques NOAA")
    csv_ingestion = NOAACSVIngestion()
    csv_path = csv_ingestion.download_noaa_data()
    
    if csv_path:
        csv_stats = csv_ingestion.ingest_csv(csv_path)
        logger.info(f"‚úÖ CSV ing√©r√©: {csv_stats['records_valid']} enregistrements valides")
    else:
        logger.error("‚ùå √âchec du t√©l√©chargement des donn√©es CSV")
        return None
    
    logger.info("\nüì• Ingestion API - Donn√©es temps r√©el")
    api_ingestion = WeatherAPIIngestion()
    api_stats = api_ingestion.ingest_api_data()
    logger.info(f"‚úÖ API ing√©r√©e: {api_stats['locations_success']} localisations")
    
    duration = time.time() - start_time
    logger.info(f"\n‚è±Ô∏è  Phase d'ingestion termin√©e en {duration:.2f}s")
    
    return csv_path


def run_processing_phase(csv_path):
    logger.info("\n" + "=" * 80)
    logger.info("PHASE 2: TRAITEMENT SPARK")
    logger.info("=" * 80)
    
    start_time = time.time()
    processor = WeatherSparkProcessor()
    
    try:
        logger.info("\n‚öôÔ∏è  Traitement des donn√©es avec Spark...")
        results = processor.process_all(csv_path)
        
        duration = time.time() - start_time
        logger.info(f"\n‚è±Ô∏è  Phase de traitement termin√©e en {duration:.2f}s")
        
        logger.info("\nüìä R√©sum√© des datasets cr√©√©s:")
        for name, df in results.items():
            count = df.count()
            logger.info(f"  - {name}: {count} enregistrements")
        
        return results, processor
        
    except Exception as e:
        logger.error(f"‚ùå Erreur lors du traitement: {e}")
        processor.stop()
        return None, None


def run_persistence_phase(results, processor):
    logger.info("\n" + "=" * 80)
    logger.info("PHASE 3: PERSISTANCE DES DONN√âES")
    logger.info("=" * 80)
    
    start_time = time.time()
    
    try:
        persistence = DataPersistence()
        
        datasets_to_save = {
            'daily': results['daily'],
            'monthly': results['monthly'],
            'seasonal': results['seasonal']
        }
        
        logger.info("\nüíæ Sauvegarde des datasets en Parquet...")
        metadata_list = persistence.save_all_datasets(datasets_to_save)
        
        duration = time.time() - start_time
        logger.info(f"\n‚è±Ô∏è  Phase de persistance termin√©e en {duration:.2f}s")
        
        logger.info("\nüì¶ Datasets sauvegard√©s:")
        for metadata in metadata_list:
            logger.info(f"  - {metadata['dataset_name']}: {metadata['size_mb']} MB")
        
        return metadata_list
        
    finally:
        if processor:
            processor.stop()
            logger.info("Session Spark arr√™t√©e")


def display_final_summary(total_duration):
    logger.info("\n" + "=" * 80)
    logger.info("üéâ PIPELINE TERMIN√â AVEC SUCC√àS")
    logger.info("=" * 80)
    logger.info(f"\n‚è±Ô∏è  Dur√©e totale: {total_duration:.2f}s ({total_duration/60:.1f} minutes)")
    logger.info("\nüìç Prochaines √©tapes:")
    logger.info("  1. V√©rifier les donn√©es dans data/processed/")
    logger.info("  2. Consulter les m√©tadonn√©es dans PostgreSQL")
    logger.info("  3. Lancer le dashboard: python dashboard/app.py")
    logger.info("  4. Acc√©der au dashboard: http://localhost:5000")
    logger.info("\nüîç Spark UI: http://localhost:8080")
    logger.info("\n" + "=" * 80)


def main():
    print_banner()
    total_start_time = time.time()
    
    try:
        csv_path = run_ingestion_phase()
        if not csv_path:
            logger.error("‚ùå √âchec de la phase d'ingestion")
            return 1
        
        results, processor = run_processing_phase(csv_path)
        if not results or not processor:
            logger.error("‚ùå √âchec de la phase de traitement")
            return 1
        
        metadata_list = run_persistence_phase(results, processor)
        if not metadata_list:
            logger.error("‚ùå √âchec de la phase de persistance")
            return 1
        
        total_duration = time.time() - total_start_time
        display_final_summary(total_duration)
        return 0
        
    except KeyboardInterrupt:
        logger.warning("\n‚ö†Ô∏è  Pipeline interrompu par l'utilisateur")
        return 1
    except Exception as e:
        logger.error(f"\n‚ùå Erreur fatale: {e}", exc_info=True)
        return 1


if __name__ == "__main__":
    sys.exit(main())

