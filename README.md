# â˜ï¸ WeatherProject - Big Data Pipeline

Pipeline d'analyse mÃ©tÃ©orologique Big Data avec Apache Spark, PostgreSQL et Flask.

## ğŸš€ Technologies

- **Apache Spark 3.3.0** - Traitement distribuÃ©
- **PostgreSQL 16** - MÃ©tadonnÃ©es
- **Flask 3.0** - Dashboard web
- **Docker** - Containerisation complÃ¨te
- **NOAA API** - DonnÃ©es mÃ©tÃ©o rÃ©elles

## ğŸ”§ DÃ©marrage Rapide

### 1. CrÃ©er le fichier .env

```env
NOAA_API_TOKEN=votre_token_ici
POSTGRES_HOST=localhost
POSTGRES_PORT=5432
POSTGRES_DB=weather_metadata
POSTGRES_USER=weather_admin
POSTGRES_PASSWORD=weather_pass_2024
```

### 2. Lancer les services Docker

```bash
docker-compose up -d
```

### 3. Lancer le pipeline

```bash
python run_pipeline.py
```

### 4. AccÃ©der au dashboard

```
http://localhost:5000
```

## ğŸ“Š Services

- **Dashboard**: http://localhost:5000
- **Spark UI**: http://localhost:8080  
- **PostgreSQL**: localhost:5432

## ğŸ“‚ Structure

```
WeatherProject/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ ingestion/      # DonnÃ©es NOAA/API
â”‚   â”œâ”€â”€ processing/     # Spark processing
â”‚   â”œâ”€â”€ persistence/    # Sauvegarde Parquet
â”‚   â””â”€â”€ utils/         # Logger, checkpoints
â”œâ”€â”€ dashboard/         # Interface Flask
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/          # DonnÃ©es brutes
â”‚   â””â”€â”€ processed/    # Parquet
â”œâ”€â”€ config/           # Configuration
â”œâ”€â”€ docker-compose.yml
â””â”€â”€ run_pipeline.py
```

## ğŸ¯ Pipeline

1. **Ingestion** - TÃ©lÃ©chargement donnÃ©es NOAA + Open-Meteo
2. **Processing** - Spark : nettoyage, agrÃ©gations, tendances
3. **Persistance** - Sauvegarde Parquet + mÃ©tadonnÃ©es PostgreSQL
4. **Dashboard** - Visualisation interactive

## ğŸ›‘ ArrÃªter

```bash
docker-compose down
```

## ğŸ“ Projet IPSSI - Big Data
